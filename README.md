# ğŸ¤– Empathetic Detection - Multimodal Emotion Recognition

This project focuses on **multimodal emotion classification** using the **AvaMERG dataset**. It combines audio, video, and text modalities to detect empathetic states with deep learning.

----

## ğŸ“ Dataset

**[AvaMERG Dataset](https://huggingface.co/datasets/ZhangHanXD/AvaMERG)**

A comprehensive multimodal emotion recognition dataset featuring:

- ï¿½ **Audio sequences** (.wav) with vocal patterns and speech features
- ï¿½ğŸ¥ **Video sequences** (.mp4) with facial expressions and gestures
- ğŸ“ **Text transcripts** with dialogue and conversation context
- ğŸ‘¤ **Speaker profiles** (age, gender, voice timbre, personality traits)
- ğŸ’­ **Empathy chains** (emotional scenarios, causes, response goals)
- ğŸ“Š **Metadata** (conversation topics, roles, temporal sequences)
- â¤ï¸ **7 Emotion Classes**: happy, surprised, angry, fear, sad, disgusted, contempt

---

## ğŸ§  Model Architecture

### **Dual Early Fusion Strategy**

We employ **two specialized early-fusion models** that will be combined via late fusion:

#### **ğŸ§ Model 1: Audio + Text + Metadata**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Audio Input   â”‚    â”‚   Text Input    â”‚    â”‚ Metadata Input  â”‚
â”‚   Wav2Vec2      â”‚    â”‚  BERT Tokens    â”‚    â”‚  Profile+Chain  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚Wav2Vec2   â”‚          â”‚BERT-base  â”‚          â”‚Embeddings â”‚
    â”‚Encoder    â”‚          â”‚ Encoder   â”‚          â”‚ Layers    â”‚
    â”‚768-dim    â”‚          â”‚768-dim    â”‚          â”‚256-dim    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚Sequential â”‚
              â”‚ LSTM +    â”‚
              â”‚ Attention â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚Classifier â”‚
              â”‚ 7 Classes â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **ğŸ¥ Model 2: Video + Text + Metadata**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Video Input   â”‚    â”‚   Text Input    â”‚    â”‚ Metadata Input  â”‚
â”‚   224x224x8     â”‚    â”‚  BERT Tokens    â”‚    â”‚  Profile+Chain  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚TimeSformerâ”‚          â”‚BERT-base  â”‚          â”‚Embeddings â”‚
    â”‚ Encoder   â”‚          â”‚ Encoder   â”‚          â”‚ Layers    â”‚
    â”‚384-dim    â”‚          â”‚768-dim    â”‚          â”‚256-dim    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚Bi-LSTM    â”‚          â”‚Bi-LSTM    â”‚          â”‚Temporal   â”‚
    â”‚+ Attentionâ”‚          â”‚+ Attentionâ”‚          â”‚Processor  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚ Fusion    â”‚
              â”‚ LSTM +    â”‚
              â”‚ Attention â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚Cross-Modalâ”‚
              â”‚ Attention â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚Classifier â”‚
              â”‚ 7 Classes â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **ğŸ”„ Planned Late Fusion Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio Model     â”‚    â”‚ Video Model     â”‚
â”‚ Predictions     â”‚    â”‚ Predictions     â”‚
â”‚ (7-dim softmax) â”‚    â”‚ (7-dim softmax) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚Late Fusionâ”‚
              â”‚ Strategy  â”‚
              â”‚(Ensemble) â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚ Final     â”‚
              â”‚Prediction â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Technical Specifications**

**ğŸ¥ Video Processing:**
- **TimeSformer Encoder**: 384-dim embeddings, 6 heads, 4 layers
- **Frame Resolution**: 224Ã—224 pixels, 8 frames per sequence
- **Patch-based**: 32Ã—32 patches with spatial-temporal attention

**ğŸ“ Text Processing:**
- **BERT-base-uncased**: 768-dim embeddings
- **Frozen Layers**: First 6 layers frozen for efficiency
- **Context**: 256 tokens, Dialogue: 128 tokens per utterance

**ğŸ“Š Metadata Processing:**
- **Speaker/Listener Profiles**: Age, gender, timbre, personality IDs
- **Empathy Chains**: Event scenarios, emotion causes, response goals
- **Rich Embeddings**: 32-dim for chain elements, 16-32 dim for profiles

**ğŸ”„ Fusion Strategy:**
- **Multi-level LSTMs**: Separate processing for each modality
- **Enhanced Attention**: 8-head multi-head attention mechanisms
- **Cross-modal Fusion**: Final attention layer for feature integration

---

## ğŸ“Š Performance Results

### **Dual Model Performance**

| Model | Modalities | Accuracy | Precision | Recall | F1-Score | Status |
|-------|------------|----------|-----------|--------|----------|---------|
| **Audio Model** | ğŸ§ğŸ“ğŸ“Š | **83.15%** | **82.76%** | **83.15%** | **82.37%** | âœ… Complete |
| **Video Model** | ğŸ¥ğŸ“ğŸ“Š | **31.89%** | **80.71%** | **31.89%** | **36.27%** | âœ… Complete |
| **Late Fusion** | ğŸ§ğŸ¥ğŸ“ğŸ“Š | **TBD** | **TBD** | **TBD** | **TBD** | ğŸ”„ Planned |

### **Audio Model Per-Class Performance** ğŸ§
| Emotion | Precision | Recall | F1-Score | Support |
|---------|-----------|--------|----------|---------|
| **Happy** | 84.2% | 84.2% | 84.2% | 1,150 |
| **Surprised** | 66.0% | 66.0% | 66.0% | 361 |
| **Angry** | 83.6% | 83.6% | 83.6% | 399 |
| **Fear** | 71.0% | 71.0% | 71.0% | 297 |
| **Sad** | 86.3% | 86.3% | 86.3% | 2,536 |
| **Disgusted** | 70.1% | 70.1% | 70.1% | 89 |
| **Contempt** | 79.0% | 79.0% | 79.0% | 107 |

### **Video Model Per-Class Performance** ğŸ¥
| Emotion | Precision | Recall | F1-Score | Support |
|---------|-----------|--------|----------|---------|
| **Happy** | 100.0% | 10.8% | 19.5% | 1,150 |
| **Surprised** | 87.8% | 23.8% | 37.5% | 361 |
| **Angry** | 16.8% | 92.2% | 28.4% | 399 |
| **Fear** | 24.1% | 44.1% | 31.2% | 297 |
| **Sad** | 93.7% | 32.3% | 48.0% | 2,536 |
| **Disgusted** | 0.0% | 0.0% | 0.0% | 89 |
| **Contempt** | 4.2% | 43.9% | 7.7% | 107 |

### **Key Insights**
- âœ… **Audio Model Excellence**: 83.15% accuracy shows strong acoustic-linguistic understanding
- âœ… **Complementary Strengths**: Video model has high precision, audio model has balanced performance
- ğŸ¯ **Fusion Potential**: Different modality strengths suggest excellent late fusion opportunities
- âš¡ **Performance Boost Expected**: Late fusion should significantly exceed individual model performance

---

## ğŸš€ Quick Start

### **1. Environment Setup**

```bash
# Clone repository
git clone https://github.com/AbdulaAlShyed-2212592042/empathetic-detection.git
cd empathetic-detection
