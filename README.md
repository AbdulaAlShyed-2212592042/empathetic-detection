# ğŸ¤– Empathetic Detection - Multimodal Emotion Recognition

This project focuses on **multimodal emotion classification** using the **AvaMERG dataset**. It combines audio, video, and text modalities to detect empathetic states with deep learning.

----

## ğŸ“ Dataset

**[AvaMERG on Hugging Face](https://huggingface.co/datasets/ZhangHanXD/AvaMERG)**

A multimodal emotion recognition dataset including:

- ğŸ§ Audio waveforms (.wav)  
- ğŸ¥ Video data  
- ğŸ“ Text transcripts  
- â¤ï¸ Empathy-related emotion labels  

----

## ğŸ¯ Objective

To build a **multimodal neural network** that detects empathy using:

- Audio features extracted via `Wav2Vec2Model`  
- Video features extracted via pretrained vision model (e.g., CNN or transformer-based)  
- Text features extracted via `BERT`  
- Fusion of all modalities for classification  

----

## ğŸ§  Model Architecture

- **Audio Encoder**: `facebook/wav2vec2-base` pretrained model  
- **Video Encoder**: (Add your chosen pretrained video model here, e.g., ResNet or Video Transformer)  
- **Text Encoder**: `bert-base-uncased`  
- **Fusion**: Concatenation of embeddings â†’ Fully connected layers â†’ Output classifier  
- **Output**: Empathy-related emotion classification  

----

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/AbdulaAlShyed-2212592042/empathetic-detection.git
cd empathetic-detection
